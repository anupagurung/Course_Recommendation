# -*- coding: utf-8 -*-
"""Final Content Based Course Recommendation by using similarity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10aDJuCk519bAV5VzD96NXcmp2qpcougG
"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import gensim
from gensim import corpora
import nltk

from scipy.spatial.distance import cosine
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import ngrams

# Load the BoW features as Pandas dataframe
bows_url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/datasets/courses_bows.csv"
bows_df = pd.read_csv(bows_url)
bows_df = bows_df[['doc_id', 'token', 'bow']]

bows_df.head(10)

# Load the course dataframe
course_url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/datasets/course_processed.csv"
course_df = pd.read_csv(course_url)

course_df.head(10)

# Inspect a specific course
course_df[course_df['COURSE_ID'] == 'ML0101ENv3']

# Get BoW features for a specific course
ml_course = bows_df[bows_df['doc_id'] == 'ML0101ENv3']
ml_course

#Define a function to pivot two BoW feature sets for comparison.
def pivot_two_bows(basedoc, comparedoc):
    base = basedoc.copy()
    base['type'] = 'base'
    compare = comparedoc.copy()
    compare['type'] = 'compare'

    # Append the two token sets vertically
    join = pd.concat([base, compare])

    # Pivot the two joined courses
    joinT = join.pivot(index=['doc_id', 'type'], columns='token').fillna(0).reset_index(level=[0, 1])

    # Assign columns
    joinT.columns = ['doc_id', 'type'] + [t[1] for t in joinT.columns][2:]
    return joinT

# Select two courses to compare
course1 = bows_df[bows_df['doc_id'] == 'ML0151EN']
course2 = bows_df[bows_df['doc_id'] == 'ML0101ENv3']

# Pivot BoW features
bow_vectors = pivot_two_bows(course1, course2)
bow_vectors

# Calculate cosine similarity
similarity = 1 - cosine(bow_vectors.iloc[0, 2:], bow_vectors.iloc[1, 2:])
similarity

# Define a threshold for similarity
#Finding similar courses based on BoW features:
threshold = 0.5
courses = course_df['COURSE_ID'].unique()
course_ML0101ENv3 = bows_df[bows_df['doc_id'] == 'ML0101ENv3']

for course_id in np.delete(courses, np.where(courses == 'ML0101ENv3')):
    course = bows_df[bows_df['doc_id'] == course_id]
    bow_vectors = pivot_two_bows(course, course_ML0101ENv3)
    similarity = 1 - cosine(bow_vectors.iloc[0, 2:], bow_vectors.iloc[1, 2:])
    if similarity >= threshold:
        print(course_id)

# Pivot the BoW data to get a matrix of courses and their BoW vectors
bows_pivot = bows_df.pivot(index='doc_id', columns='token', values='bow').fillna(0)

# Import the necessary function
from sklearn.metrics.pairwise import cosine_similarity

# Calculate the cosine similarity matrix for courses using BoW features
course_similarity = cosine_similarity(bows_pivot)
course_ids = bows_pivot.index

# Create a DataFrame to store the similarities
course_similarity_df = pd.DataFrame(course_similarity, index=course_ids, columns=course_ids)

# Function to get top N similar courses based on a given course_id
def get_top_n_similar_courses(course_id, n=10):
    if course_id not in course_similarity_df.index:
        return []
    similar_courses = course_similarity_df.loc[course_id].sort_values(ascending=False).index[1:n+1].tolist()
    return similar_courses

# Generate recommendations for each course
course_recommendations = {}

# Iterate through each course in the course dataframe
for course_id in course_ids:
    # Find top N similar courses
    similar_courses = get_top_n_similar_courses(course_id, n=10)
    course_recommendations[course_id] = similar_courses

# Convert the recommendations dictionary into a DataFrame
recommendations_df = pd.DataFrame.from_dict(course_recommendations, orient='index')
recommendations_df.columns = [f'Recommended Course {i+1}' for i in range(recommendations_df.shape[1])]

# Save the recommendations to a CSV file
recommendations_df.to_csv('course_recommendations.csv', index_label='course_id')

print("Top 10 course recommendations for each course have been saved to 'course_recommendations.csv'")

import pickle

with open('Recommendation.pkl', 'wb') as file:
  pickle.dump(course_recommendations,file)